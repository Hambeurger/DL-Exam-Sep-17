{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hambeurger/Deep-Learning-Project-2/blob/main/Deep_Learning_Project_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import all the libraries."
      ],
      "metadata": {
        "id": "N0s78M6IGDIg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "import zipfile\n",
        "import pickle as pk\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "hl8Uxww_GRXg"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Data Preparation: Download and Load the Data.\n",
        "\n",
        "The data is a 2D numpy array, where each vector is of size 1000."
      ],
      "metadata": {
        "id": "0pg23O9-GTDn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "zip_url = \"https://github.com/Hambeurger/Deep-Learning-Project-2/raw/main/input_data.zip\"\n",
        "zip_path, _ = urllib.request.urlretrieve(zip_url)\n",
        "target_folder = \"temp\"\n",
        "\n",
        "# Unzipping the downloaded file\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(target_folder)\n",
        "\n",
        "# Loading the data\n",
        "data_path = f\"{target_folder}/input_data.pkl\"\n",
        "with open(data_path, 'rb') as f:\n",
        "    dd = pk.load(f)\n",
        "data = dd['data']  # Data has a shape of (150, 1000)"
      ],
      "metadata": {
        "id": "faDvnTMEGYxH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Dataset Preparation\n",
        "\n",
        "Generate corrupted versions of the input data to train the denoising autoencoder."
      ],
      "metadata": {
        "id": "MiwSFLoMGiXA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    # Randomly corrupt the input by flipping or altering some values\n",
        "    def corrupt_vector(self, vector):\n",
        "\n",
        "        corruption_type = np.random.choice(['flip', 'alter'], p=[0.5, 0.5])\n",
        "        corrupted_vector = np.copy(vector)\n",
        "\n",
        "        if corruption_type == 'flip':\n",
        "            flip_indices = np.random.choice(len(vector), size=int(0.1 * len(vector)), replace=False)\n",
        "            corrupted_vector[flip_indices] *= -1\n",
        "        elif corruption_type == 'alter':\n",
        "            alter_indices = np.random.choice(len(vector), size=int(0.1 * len(vector)), replace=False)\n",
        "            corrupted_vector[alter_indices] = np.random.uniform(-1, 1, size=alter_indices.shape)\n",
        "\n",
        "        return corrupted_vector\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    # Returns both the corrupted vector (input to the model) and the original vector (target output)\n",
        "    def __getitem__(self, idx):\n",
        "        original_vector = self.data[idx]\n",
        "        corrupted_vector = self.corrupt_vector(original_vector)\n",
        "        return torch.tensor(corrupted_vector, dtype=torch.float32), torch.tensor(original_vector, dtype=torch.float32)\n",
        "\n",
        "# Create dataset and DataLoader\n",
        "dataset = CustomDataset(data)\n",
        "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)"
      ],
      "metadata": {
        "id": "Lr1CuX_TGoAf"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Model Architecture - Denoising Autoencoder\n",
        "\n",
        "A denoising autoencoder is built using fully connected layers. The encoder compresses the input into a lower-dimensional representation (128 neurons in the bottleneck), and the decoder reconstructs it back to the original dimension (1000 neurons)."
      ],
      "metadata": {
        "id": "Bg9juOSgGsO3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DenoisingAutoencoder(nn.Module):\n",
        "    def __init__(self, input_size=1000, hidden_size=512):\n",
        "        super(DenoisingAutoencoder, self).__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, 128),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(128, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, input_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encode and decode process\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "\n",
        "        # Apply sign function to ensure output in {-1, 1}\n",
        "        return torch.sign(decoded)\n",
        "\n",
        "# Instantiate the model\n",
        "model = DenoisingAutoencoder()"
      ],
      "metadata": {
        "id": "MhfTntUQHN1R"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Training process\n",
        "\n",
        "The model is trained using Mean Squared Error (MSE) loss, which measures the reconstruction error between the original and reconstructed vectors. The Adam optimizer is used to update the weights. The network is trained for 50 epochs, printing the average loss at the end of each epoch."
      ],
      "metadata": {
        "id": "hsoWx39DHRio"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training setup\n",
        "criterion = nn.MSELoss()  # Mean Squared Error loss to minimize reconstruction error\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "num_epochs = 50\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0.0\n",
        "    for corrupted_vectors, original_vectors in dataloader:\n",
        "        # Zero the gradient buffers\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass: Get model outputs\n",
        "        outputs = model(corrupted_vectors)\n",
        "\n",
        "        # Calculate the loss\n",
        "        loss = criterion(outputs, original_vectors)\n",
        "\n",
        "        # Backpropagate the loss\n",
        "        loss.backward()\n",
        "\n",
        "        # Update the weights\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFzhGTmyHTyx",
        "outputId": "04250f2b-3ee6-4774-ba66-a9c088ff3743"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], Loss: 1.9975\n",
            "Epoch [2/50], Loss: 2.0015\n",
            "Epoch [3/50], Loss: 2.0020\n",
            "Epoch [4/50], Loss: 2.0027\n",
            "Epoch [5/50], Loss: 2.0019\n",
            "Epoch [6/50], Loss: 2.0046\n",
            "Epoch [7/50], Loss: 1.9984\n",
            "Epoch [8/50], Loss: 1.9985\n",
            "Epoch [9/50], Loss: 2.0010\n",
            "Epoch [10/50], Loss: 2.0021\n",
            "Epoch [11/50], Loss: 2.0088\n",
            "Epoch [12/50], Loss: 2.0028\n",
            "Epoch [13/50], Loss: 2.0079\n",
            "Epoch [14/50], Loss: 2.0025\n",
            "Epoch [15/50], Loss: 1.9988\n",
            "Epoch [16/50], Loss: 2.0000\n",
            "Epoch [17/50], Loss: 2.0007\n",
            "Epoch [18/50], Loss: 2.0015\n",
            "Epoch [19/50], Loss: 2.0055\n",
            "Epoch [20/50], Loss: 2.0054\n",
            "Epoch [21/50], Loss: 2.0056\n",
            "Epoch [22/50], Loss: 2.0031\n",
            "Epoch [23/50], Loss: 2.0020\n",
            "Epoch [24/50], Loss: 2.0018\n",
            "Epoch [25/50], Loss: 2.0036\n",
            "Epoch [26/50], Loss: 1.9998\n",
            "Epoch [27/50], Loss: 2.0074\n",
            "Epoch [28/50], Loss: 2.0022\n",
            "Epoch [29/50], Loss: 2.0023\n",
            "Epoch [30/50], Loss: 1.9978\n",
            "Epoch [31/50], Loss: 1.9969\n",
            "Epoch [32/50], Loss: 2.0067\n",
            "Epoch [33/50], Loss: 2.0033\n",
            "Epoch [34/50], Loss: 2.0035\n",
            "Epoch [35/50], Loss: 2.0067\n",
            "Epoch [36/50], Loss: 2.0049\n",
            "Epoch [37/50], Loss: 2.0008\n",
            "Epoch [38/50], Loss: 1.9986\n",
            "Epoch [39/50], Loss: 2.0011\n",
            "Epoch [40/50], Loss: 2.0011\n",
            "Epoch [41/50], Loss: 2.0047\n",
            "Epoch [42/50], Loss: 1.9971\n",
            "Epoch [43/50], Loss: 2.0023\n",
            "Epoch [44/50], Loss: 2.0000\n",
            "Epoch [45/50], Loss: 2.0048\n",
            "Epoch [46/50], Loss: 2.0017\n",
            "Epoch [47/50], Loss: 2.0021\n",
            "Epoch [48/50], Loss: 2.0032\n",
            "Epoch [49/50], Loss: 2.0050\n",
            "Epoch [50/50], Loss: 2.0013\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Evaluation of the model\n",
        "\n",
        "After training, the model is evaluated using the same MSE loss function, gradient updates are disabled (with torch.no_grad()) to speed up evaluation and save memory. Measure accuracy to ensure exact values in {-1,1}."
      ],
      "metadata": {
        "id": "6JkVMPc9HaS4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, dataset, criterion):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    total_loss = 0.0\n",
        "    total_accuracy = 0.0\n",
        "    total_samples = 0\n",
        "    with torch.no_grad():  # Disable gradient computation for evaluation\n",
        "        for corrupted_vectors, original_vectors in dataset:\n",
        "            outputs = model(corrupted_vectors)\n",
        "            loss = criterion(outputs, original_vectors)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Accuracy: Compare the output with original\n",
        "            predicted = torch.sign(outputs)\n",
        "            correct_matches = (predicted == original_vectors).sum().item()\n",
        "            total_accuracy += correct_matches\n",
        "            total_samples += original_vectors.numel()  # Total number of elements\n",
        "\n",
        "    avg_loss = total_loss / len(dataset)\n",
        "    accuracy = total_accuracy / total_samples\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "# Model evaluation on the dataset\n",
        "test_loss, accuracy = evaluate_model(model, dataset, criterion)\n",
        "print(f'Test Loss (Reconstruction Error): {test_loss:.4f}')\n",
        "print(f'Accuracy (Exact Recovery): {accuracy:.4%}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fUzVzm9HdiA",
        "outputId": "ffb7f513-8b71-426d-b212-73f050a639f6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss (Reconstruction Error): 1.9998\n",
            "Accuracy (Exact Recovery): 50.0060%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Model saving\n",
        "\n",
        "The trained model's parameters are saved in a file denoising_autoencoder_model.pth for future use."
      ],
      "metadata": {
        "id": "EfjyJZoLHgT3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "iGp1EP6dDIGS"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), 'denoising_autoencoder_model.pth')"
      ]
    }
  ]
}